{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "986b01b5",
   "metadata": {},
   "source": [
    "# Pr√©diction de Consommation √ânerg√©tique - Campus Aix\n",
    "\n",
    "## Objectifs du projet\n",
    "\n",
    "Ce notebook pr√©sente un pipeline complet de pr√©diction de consommation √©nerg√©tique horaire pour un campus, en utilisant :\n",
    "- Des donn√©es temporelles (heure, jour, mois, ann√©e)\n",
    "- Des variables m√©t√©orologiques\n",
    "- Des features d√©riv√©es (lags, diff√©rences, encodages cycliques)\n",
    "- Un mod√®le de r√©seau de neurones (MLPRegressor)\n",
    "\n",
    "**Objectif m√©tier** : Pr√©dire la consommation √©nerg√©tique horaire pour optimiser la gestion des ressources.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf2b3a7",
   "metadata": {},
   "source": [
    "## 1. Imports et Configuration\n",
    "\n",
    "Importation des biblioth√®ques n√©cessaires et configuration de la reproductibilit√©.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334eeb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuration pour la reproductibilit√©\n",
    "np.random.seed(42)\n",
    "\n",
    "# Configuration des graphiques\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "\n",
    "print(\"‚úì Imports effectu√©s avec succ√®s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c5e259",
   "metadata": {},
   "source": [
    "## 2. Chargement des Donn√©es\n",
    "\n",
    "Chargement du fichier CSV contenant les donn√©es de consommation et les variables explicatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c7fc9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des donn√©es\n",
    "data = np.genfromtxt('conso_campus_aix.csv', delimiter=',', skip_header=1)\n",
    "\n",
    "# Extraction de la variable cible (colonne 8 : consommation)\n",
    "conso = data[:, 8]\n",
    "y_orig = conso\n",
    "\n",
    "# Extraction des variables temporelles\n",
    "Jours_du_mois = data[:, 0].astype(int)\n",
    "Mois_orig = data[:, 1].astype(int)\n",
    "Annees_orig = data[:, 2].astype(int)\n",
    "Heure_orig = data[:, 3].astype(int)\n",
    "\n",
    "# Variables m√©t√©orologiques (colonnes 4, 5, 6, 7)\n",
    "variables_meteo = data[:, [4, 5, 6, 7]]\n",
    "\n",
    "print(f\"‚úì Donn√©es charg√©es : {len(data)} observations\")\n",
    "print(f\"  - Consommation : min={y_orig.min():.2f}, max={y_orig.max():.2f}, mean={y_orig.mean():.2f}\")\n",
    "print(f\"  - P√©riode : {Annees_orig.min()}-{Mois_orig.min():02d} √† {Annees_orig.max()}-{Mois_orig.max():02d}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e16174",
   "metadata": {},
   "source": [
    "## 3. Exploration des Donn√©es (EDA)\n",
    "\n",
    "### 3.1. Analyse de la variable cible\n",
    "\n",
    "Visualisation de la distribution et de l'√©volution temporelle de la consommation pour identifier :\n",
    "- Les tendances et saisonnalit√©s\n",
    "- Les valeurs aberrantes potentielles\n",
    "- La distribution des valeurs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7fc532a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de l'√©volution temporelle de la consommation\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# S√©rie temporelle compl√®te\n",
    "axes[0, 0].plot(y_orig[:1000], alpha=0.7, linewidth=0.5)\n",
    "axes[0, 0].set_title('√âvolution de la consommation (1000 premi√®res heures)')\n",
    "axes[0, 0].set_xlabel('Heures')\n",
    "axes[0, 0].set_ylabel('Consommation')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution\n",
    "axes[0, 1].hist(y_orig, bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_title('Distribution de la consommation')\n",
    "axes[0, 1].set_xlabel('Consommation')\n",
    "axes[0, 1].set_ylabel('Fr√©quence')\n",
    "axes[0, 1].axvline(y_orig.mean(), color='r', linestyle='--', label=f'Moyenne: {y_orig.mean():.2f}')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Box plot pour d√©tecter les outliers\n",
    "axes[1, 0].boxplot(y_orig, vert=True)\n",
    "axes[1, 0].set_title('Box Plot - D√©tection des valeurs aberrantes')\n",
    "axes[1, 0].set_ylabel('Consommation')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Statistiques descriptives\n",
    "stats_text = f\"\"\"\n",
    "Statistiques descriptives :\n",
    "- Min: {y_orig.min():.2f}\n",
    "- Max: {y_orig.max():.2f}\n",
    "- Moyenne: {y_orig.mean():.2f}\n",
    "- M√©diane: {np.median(y_orig):.2f}\n",
    "- √âcart-type: {y_orig.std():.2f}\n",
    "- Skewness: {np.abs(y_orig - y_orig.mean()).mean() / y_orig.std():.2f}\n",
    "\"\"\"\n",
    "axes[1, 1].text(0.1, 0.5, stats_text, fontsize=11, verticalalignment='center',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "axes[1, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433fa0c1",
   "metadata": {},
   "source": [
    "### 3.2. Analyse des patterns temporels\n",
    "\n",
    "Analyse de la consommation selon l'heure, le jour de la semaine et le mois pour identifier les patterns cycliques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cec2ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des jours de la semaine pour l'analyse\n",
    "jours_de_la_semaine_temp = []\n",
    "for annee, mois, jour in zip(Annees_orig, Mois_orig, Jours_du_mois):\n",
    "    date_obj = datetime.date(annee, mois, jour)\n",
    "    jours_de_la_semaine_temp.append(date_obj.weekday())\n",
    "jours_de_la_semaine_temp = np.array(jours_de_la_semaine_temp)\n",
    "\n",
    "# Visualisation des patterns temporels\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Consommation par heure\n",
    "conso_par_heure = [y_orig[Heure_orig == h].mean() for h in range(24)]\n",
    "axes[0, 0].plot(range(24), conso_par_heure, marker='o', linewidth=2, markersize=6)\n",
    "axes[0, 0].set_title('Consommation moyenne par heure de la journ√©e')\n",
    "axes[0, 0].set_xlabel('Heure')\n",
    "axes[0, 0].set_ylabel('Consommation moyenne')\n",
    "axes[0, 0].set_xticks(range(0, 24, 2))\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Consommation par jour de la semaine\n",
    "jours_noms = ['Lun', 'Mar', 'Mer', 'Jeu', 'Ven', 'Sam', 'Dim']\n",
    "conso_par_jour = [y_orig[jours_de_la_semaine_temp == j].mean() for j in range(7)]\n",
    "axes[0, 1].bar(jours_noms, conso_par_jour, color='steelblue', alpha=0.7)\n",
    "axes[0, 1].set_title('Consommation moyenne par jour de la semaine')\n",
    "axes[0, 1].set_ylabel('Consommation moyenne')\n",
    "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Consommation par mois\n",
    "conso_par_mois = [y_orig[Mois_orig == m].mean() for m in range(1, 13)]\n",
    "mois_noms = ['Jan', 'F√©v', 'Mar', 'Avr', 'Mai', 'Jun', \n",
    "             'Jul', 'Ao√ª', 'Sep', 'Oct', 'Nov', 'D√©c']\n",
    "axes[1, 0].plot(range(1, 13), conso_par_mois, marker='o', linewidth=2, markersize=6, color='green')\n",
    "axes[1, 0].set_title('Consommation moyenne par mois')\n",
    "axes[1, 0].set_xlabel('Mois')\n",
    "axes[1, 0].set_ylabel('Consommation moyenne')\n",
    "axes[1, 0].set_xticks(range(1, 13))\n",
    "axes[1, 0].set_xticklabels(mois_noms, rotation=45)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Heatmap consommation par heure et jour de la semaine\n",
    "heatmap_data = np.zeros((7, 24))\n",
    "for j in range(7):\n",
    "    for h in range(24):\n",
    "        mask = (jours_de_la_semaine_temp == j) & (Heure_orig == h)\n",
    "        if mask.sum() > 0:\n",
    "            heatmap_data[j, h] = y_orig[mask].mean()\n",
    "im = axes[1, 1].imshow(heatmap_data, aspect='auto', cmap='YlOrRd', interpolation='nearest')\n",
    "axes[1, 1].set_title('Heatmap : Consommation par heure et jour de la semaine')\n",
    "axes[1, 1].set_xlabel('Heure')\n",
    "axes[1, 1].set_ylabel('Jour de la semaine')\n",
    "axes[1, 1].set_yticks(range(7))\n",
    "axes[1, 1].set_yticklabels(jours_noms)\n",
    "axes[1, 1].set_xticks(range(0, 24, 2))\n",
    "plt.colorbar(im, ax=axes[1, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89f5a62",
   "metadata": {},
   "source": [
    "### 3.3. Analyse des corr√©lations et valeurs manquantes\n",
    "\n",
    "V√©rification des corr√©lations entre variables m√©t√©orologiques et la consommation, ainsi que d√©tection des valeurs manquantes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77204c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rification des valeurs manquantes\n",
    "missing_data = np.isnan(data).sum(axis=0)\n",
    "print(\"Valeurs manquantes par colonne :\")\n",
    "for i, missing in enumerate(missing_data):\n",
    "    if missing > 0:\n",
    "        print(f\"  Colonne {i}: {missing} valeurs manquantes ({missing/len(data)*100:.2f}%)\")\n",
    "    else:\n",
    "        print(f\"  Colonne {i}: Aucune valeur manquante\")\n",
    "\n",
    "# Analyse des corr√©lations avec les variables m√©t√©orologiques\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Corr√©lations entre variables m√©t√©orologiques et consommation\n",
    "corr_with_target = []\n",
    "for i in range(4):\n",
    "    corr = np.corrcoef(variables_meteo[:, i], y_orig)[0, 1]\n",
    "    corr_with_target.append(corr)\n",
    "\n",
    "axes[0].bar(range(1, 5), corr_with_target, color='coral', alpha=0.7)\n",
    "axes[0].set_title('Corr√©lation entre variables m√©t√©o et consommation')\n",
    "axes[0].set_xlabel('Variable m√©t√©o (colonne)')\n",
    "axes[0].set_ylabel('Corr√©lation avec consommation')\n",
    "axes[0].set_xticks(range(1, 5))\n",
    "axes[0].axhline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Distribution des variables m√©t√©orologiques\n",
    "axes[1].boxplot([variables_meteo[:, i] for i in range(4)], labels=[f'Var {i+4}' for i in range(4)])\n",
    "axes[1].set_title('Distribution des variables m√©t√©orologiques')\n",
    "axes[1].set_ylabel('Valeurs')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n‚úì Corr√©lations variables m√©t√©o ‚Üí consommation : {[f'{c:.3f}' for c in corr_with_target]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a074ee93",
   "metadata": {},
   "source": [
    "### üìä Interpr√©tation & Pistes d'am√©lioration - EDA\n",
    "\n",
    "**Observations cl√©s :**\n",
    "- Distribution de la consommation : identifier si elle est normale, asym√©trique, ou pr√©sente des outliers\n",
    "- Patterns temporels : heures de pointe, diff√©rences week-end/semaine, saisonnalit√©\n",
    "- Corr√©lations m√©t√©o : quelles variables m√©t√©orologiques sont les plus pr√©dictives\n",
    "\n",
    "**Pistes d'am√©lioration :**\n",
    "- Appliquer une transformation (log, Box-Cox) si la distribution est asym√©trique\n",
    "- Cr√©er des features d'interaction entre variables m√©t√©o et variables temporelles\n",
    "- G√©rer les outliers si n√©cessaire (capping, suppression, ou mod√©lisation s√©par√©e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d079233",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering\n",
    "\n",
    "### 4.1. Encodages cycliques\n",
    "\n",
    "Les variables temporelles (heure, mois, jour de la semaine) sont cycliques. L'encodage sinuso√Ødal permet au mod√®le de comprendre cette cyclicit√© (ex: 23h est proche de 0h).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e45ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encodage cyclique de l'heure (24h)\n",
    "H_sin = np.sin(2 * np.pi * Heure_orig / 24)\n",
    "H_cos = np.cos(2 * np.pi * Heure_orig / 24)\n",
    "\n",
    "# Encodage cyclique du mois (12 mois)\n",
    "M_sin = np.sin(2 * np.pi * Mois_orig / 12)\n",
    "M_cos = np.cos(2 * np.pi * Mois_orig / 12)\n",
    "\n",
    "# Calcul du jour de la semaine\n",
    "jours_de_la_semaine = []\n",
    "for annee, mois, jour in zip(Annees_orig, Mois_orig, Jours_du_mois):\n",
    "    date_obj = datetime.date(annee, mois, jour)\n",
    "    jours_de_la_semaine.append(date_obj.weekday())\n",
    "jours_de_la_semaine = np.array(jours_de_la_semaine)\n",
    "\n",
    "# Encodage cyclique du jour de la semaine (7 jours)\n",
    "S_sin = np.sin(2 * np.pi * jours_de_la_semaine / 7)\n",
    "S_cos = np.cos(2 * np.pi * jours_de_la_semaine / 7)\n",
    "\n",
    "print(\"‚úì Encodages cycliques cr√©√©s\")\n",
    "print(f\"  - Heure: sin/cos (24h cycle)\")\n",
    "print(f\"  - Mois: sin/cos (12 mois cycle)\")\n",
    "print(f\"  - Jour semaine: sin/cos (7 jours cycle)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6f8903",
   "metadata": {},
   "source": [
    "### 4.2. Visualisation des encodages cycliques\n",
    "\n",
    "Visualisation pour v√©rifier que les encodages capturent bien la cyclicit√© des variables temporelles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000bf146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des encodages cycliques\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "\n",
    "# Encodage heure\n",
    "axes[0].scatter(H_sin[:1000], H_cos[:1000], c=Heure_orig[:1000], cmap='hsv', alpha=0.6, s=10)\n",
    "axes[0].set_title('Encodage cyclique de l\\'heure (sin/cos)')\n",
    "axes[0].set_xlabel('sin(2œÄ * heure / 24)')\n",
    "axes[0].set_ylabel('cos(2œÄ * heure / 24)')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_aspect('equal')\n",
    "\n",
    "# Encodage mois\n",
    "axes[1].scatter(M_sin[:1000], M_cos[:1000], c=Mois_orig[:1000], cmap='hsv', alpha=0.6, s=10)\n",
    "axes[1].set_title('Encodage cyclique du mois (sin/cos)')\n",
    "axes[1].set_xlabel('sin(2œÄ * mois / 12)')\n",
    "axes[1].set_ylabel('cos(2œÄ * mois / 12)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_aspect('equal')\n",
    "\n",
    "# Encodage jour de la semaine\n",
    "axes[2].scatter(S_sin[:1000], S_cos[:1000], c=jours_de_la_semaine[:1000], cmap='hsv', alpha=0.6, s=10)\n",
    "axes[2].set_title('Encodage cyclique du jour de la semaine (sin/cos)')\n",
    "axes[2].set_xlabel('sin(2œÄ * jour / 7)')\n",
    "axes[2].set_ylabel('cos(2œÄ * jour / 7)')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da1be5b6",
   "metadata": {},
   "source": [
    "### 4.3. Variables binaires et cat√©gorielles\n",
    "\n",
    "Cr√©ation de variables binaires pour capturer des effets sp√©cifiques (week-end, heures actives).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a889043",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable binaire Week-end (1 si samedi ou dimanche, 0 sinon)\n",
    "is_weekend = (jours_de_la_semaine >= 5).astype(int)\n",
    "\n",
    "# Variable binaire Heures actives (1 si entre 7h et 20h, 0 sinon)\n",
    "# Cette variable aide le mod√®le √† mieux caler le \"talon\" de consommation nocturne\n",
    "is_active_hours = ((Heure_orig >= 7) & (Heure_orig <= 20)).astype(int)\n",
    "\n",
    "# Visualisation de l'impact de ces variables\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Impact du week-end\n",
    "conso_weekend = y_orig[is_weekend == 1].mean()\n",
    "conso_semaine = y_orig[is_weekend == 0].mean()\n",
    "axes[0].bar(['Semaine', 'Week-end'], [conso_semaine, conso_weekend], \n",
    "            color=['steelblue', 'coral'], alpha=0.7)\n",
    "axes[0].set_title('Consommation moyenne : Semaine vs Week-end')\n",
    "axes[0].set_ylabel('Consommation moyenne')\n",
    "axes[0].grid(True, alpha=0.3, axis='y')\n",
    "diff_pct = ((conso_weekend - conso_semaine) / conso_semaine) * 100\n",
    "axes[0].text(0.5, max(conso_semaine, conso_weekend) * 0.95, \n",
    "             f'Diff√©rence: {diff_pct:.1f}%', ha='center', fontsize=10,\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "# Impact des heures actives\n",
    "conso_active = y_orig[is_active_hours == 1].mean()\n",
    "conso_inactive = y_orig[is_active_hours == 0].mean()\n",
    "axes[1].bar(['Heures inactives\\n(21h-6h)', 'Heures actives\\n(7h-20h)'], \n",
    "            [conso_inactive, conso_active], color=['darkblue', 'orange'], alpha=0.7)\n",
    "axes[1].set_title('Consommation moyenne : Heures actives vs inactives')\n",
    "axes[1].set_ylabel('Consommation moyenne')\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "diff_pct2 = ((conso_active - conso_inactive) / conso_inactive) * 100\n",
    "axes[1].text(0.5, max(conso_active, conso_inactive) * 0.95, \n",
    "             f'Diff√©rence: {diff_pct2:.1f}%', ha='center', fontsize=10,\n",
    "             bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Variables binaires cr√©√©es\")\n",
    "print(f\"  - Week-end: {is_weekend.sum()} observations ({is_weekend.sum()/len(is_weekend)*100:.1f}%)\")\n",
    "print(f\"  - Heures actives: {is_active_hours.sum()} observations ({is_active_hours.sum()/len(is_active_hours)*100:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7709a32b",
   "metadata": {},
   "source": [
    "### 4.4. Features temporelles avanc√©es (Lags et diff√©rences)\n",
    "\n",
    "Les lags (valeurs pass√©es) et les diff√©rences capturent l'inertie et les tendances de la consommation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91cd0009",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cr√©ation des lags (valeurs pass√©es)\n",
    "lag_1 = np.roll(y_orig, 1)   # Consommation 1h avant\n",
    "lag_2 = np.roll(y_orig, 2)   # Consommation 2h avant\n",
    "lag_24 = np.roll(y_orig, 24) # Consommation 24h avant (m√™me heure hier)\n",
    "lag_168 = np.roll(y_orig, 168) # Consommation 168h avant (m√™me heure la semaine derni√®re)\n",
    "\n",
    "# Cr√©ation des diff√©rences (tendances)\n",
    "diff_1h = lag_1 - lag_2      # Variation sur 1h\n",
    "diff_24h = lag_1 - lag_24    # Variation sur 24h\n",
    "\n",
    "# Visualisation de l'importance des lags\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Corr√©lation avec la cible\n",
    "lags = [lag_1, lag_24, lag_168]\n",
    "lag_names = ['Lag 1h', 'Lag 24h', 'Lag 168h']\n",
    "correlations = [np.corrcoef(y_orig, lag)[0, 1] for lag in lags]\n",
    "\n",
    "axes[0, 0].bar(lag_names, correlations, color='teal', alpha=0.7)\n",
    "axes[0, 0].set_title('Corr√©lation des lags avec la consommation actuelle')\n",
    "axes[0, 0].set_ylabel('Corr√©lation')\n",
    "axes[0, 0].set_ylim([0, 1])\n",
    "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Visualisation des lags sur un extrait\n",
    "sample_idx = slice(200, 400)\n",
    "axes[0, 1].plot(y_orig[sample_idx], label='Consommation actuelle', linewidth=2)\n",
    "axes[0, 1].plot(lag_1[sample_idx], label='Lag 1h', alpha=0.7, linestyle='--')\n",
    "axes[0, 1].plot(lag_24[sample_idx], label='Lag 24h', alpha=0.7, linestyle=':')\n",
    "axes[0, 1].set_title('Comparaison consommation actuelle vs lags (zoom 200h)')\n",
    "axes[0, 1].set_xlabel('Heures')\n",
    "axes[0, 1].set_ylabel('Consommation')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Visualisation des diff√©rences\n",
    "axes[1, 0].plot(diff_1h[sample_idx], label='Diff√©rence 1h', alpha=0.7, linewidth=1.5)\n",
    "axes[1, 0].plot(diff_24h[sample_idx], label='Diff√©rence 24h', alpha=0.7, linewidth=1.5)\n",
    "axes[1, 0].axhline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[1, 0].set_title('Variations temporelles (diff√©rences)')\n",
    "axes[1, 0].set_xlabel('Heures')\n",
    "axes[1, 0].set_ylabel('Diff√©rence')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Scatter plot lag_24 vs consommation (tr√®s corr√©l√©)\n",
    "axes[1, 1].scatter(lag_24[168:], y_orig[168:], alpha=0.3, s=5)\n",
    "axes[1, 1].plot([y_orig.min(), y_orig.max()], [y_orig.min(), y_orig.max()], \n",
    "                'r--', lw=2, label='y=x')\n",
    "axes[1, 1].set_title(f'Lag 24h vs Consommation actuelle (corr={correlations[1]:.3f})')\n",
    "axes[1, 1].set_xlabel('Consommation il y a 24h')\n",
    "axes[1, 1].set_ylabel('Consommation actuelle')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Features temporelles cr√©√©es\")\n",
    "print(f\"  - Lags: 1h, 2h, 24h, 168h\")\n",
    "print(f\"  - Diff√©rences: 1h, 24h\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce04e7a",
   "metadata": {},
   "source": [
    "### üìä Interpr√©tation & Pistes d'am√©lioration - Feature Engineering\n",
    "\n",
    "**Observations cl√©s :**\n",
    "- Les lags (surtout lag_24h) sont tr√®s corr√©l√©s avec la consommation actuelle ‚Üí indicateurs puissants\n",
    "- Les encodages cycliques capturent bien les patterns temporels\n",
    "- Les variables binaires (week-end, heures actives) montrent des diff√©rences significatives\n",
    "\n",
    "**Pistes d'am√©lioration :**\n",
    "- Ajouter des lags suppl√©mentaires (lag_48h, lag_336h pour capturer des cycles plus longs)\n",
    "- Cr√©er des features d'interaction (ex: week-end √ó heure, m√©t√©o √ó saison)\n",
    "- Tester des moyennes mobiles (rolling mean) pour lisser les variations\n",
    "- Ajouter des features de tendance (d√©riv√©e seconde, acc√©l√©ration)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b17914a",
   "metadata": {},
   "source": [
    "## 5. Pr√©paration des Donn√©es pour le Mod√®le\n",
    "\n",
    "### 5.1. Construction de la matrice de features\n",
    "\n",
    "Assemblage de toutes les features cr√©√©es en une matrice X et pr√©paration de la variable cible y.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b01fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construction de la matrice de features compl√®te\n",
    "X_full = np.column_stack((\n",
    "    variables_meteo,           # 4 variables m√©t√©orologiques\n",
    "    M_sin, M_cos,              # Encodage cyclique mois (2 features)\n",
    "    H_sin, H_cos,              # Encodage cyclique heure (2 features)\n",
    "    S_sin, S_cos,              # Encodage cyclique jour semaine (2 features)\n",
    "    is_weekend,                # Variable binaire week-end (1 feature)\n",
    "    is_active_hours,           # Variable binaire heures actives (1 feature)\n",
    "    lag_1, lag_24, lag_168,    # Lags temporels (3 features)\n",
    "    diff_1h, diff_24h          # Diff√©rences temporelles (2 features)\n",
    "))\n",
    "\n",
    "# Suppression des premi√®res lignes (n√©cessaires pour les lags)\n",
    "# On garde seulement les observations o√π tous les lags sont disponibles\n",
    "X = X_full[168:]\n",
    "y = y_orig[168:]\n",
    "\n",
    "print(f\"‚úì Matrice de features construite\")\n",
    "print(f\"  - Nombre de features: {X.shape[1]}\")\n",
    "print(f\"  - Nombre d'observations: {X.shape[0]} (apr√®s suppression des 168 premi√®res)\")\n",
    "print(f\"  - Features: {X.shape[1]} variables\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3050a1d2",
   "metadata": {},
   "source": [
    "### 5.2. Transformation de la variable cible\n",
    "\n",
    "Application d'une transformation logarithmique (log1p) pour stabiliser la variance et am√©liorer les performances du mod√®le sur des donn√©es potentiellement asym√©triques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8841335",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformation logarithmique de la variable cible\n",
    "y_log = np.log1p(y)  # log1p(x) = log(1+x) pour g√©rer les valeurs proches de 0\n",
    "\n",
    "# Visualisation de l'impact de la transformation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Distribution avant transformation\n",
    "axes[0].hist(y, bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0].set_title('Distribution de y (avant transformation)')\n",
    "axes[0].set_xlabel('Consommation')\n",
    "axes[0].set_ylabel('Fr√©quence')\n",
    "axes[0].axvline(y.mean(), color='r', linestyle='--', label=f'Moyenne: {y.mean():.2f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Distribution apr√®s transformation\n",
    "axes[1].hist(y_log, bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
    "axes[1].set_title('Distribution de y_log (apr√®s log1p)')\n",
    "axes[1].set_xlabel('log(1 + Consommation)')\n",
    "axes[1].set_ylabel('Fr√©quence')\n",
    "axes[1].axvline(y_log.mean(), color='r', linestyle='--', label=f'Moyenne: {y_log.mean():.2f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Comparaison des statistiques\n",
    "print(\"Statistiques avant/apr√®s transformation :\")\n",
    "print(f\"  Avant - Skewness: {np.abs(y - y.mean()).mean() / y.std():.3f}\")\n",
    "print(f\"  Apr√®s - Skewness: {np.abs(y_log - y_log.mean()).mean() / y_log.std():.3f}\")\n",
    "print(f\"  ‚úì Transformation appliqu√©e pour stabiliser la variance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac687591",
   "metadata": {},
   "source": [
    "### 5.3. Split train/test s√©quentiel\n",
    "\n",
    "Pour les donn√©es temporelles, on utilise un split s√©quentiel (pas de shuffle) pour pr√©server l'ordre chronologique.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb37c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split s√©quentiel (80% train, 20% test)\n",
    "test_size = 0.2\n",
    "split_index = int(len(X) * (1 - test_size))\n",
    "\n",
    "X_train, X_test = X[:split_index], X[split_index:]\n",
    "y_train_log, y_test_log = y_log[:split_index], y_log[split_index:]\n",
    "y_test_real = y[split_index:]  # On garde aussi les valeurs r√©elles pour l'√©valuation\n",
    "\n",
    "print(f\"‚úì Split s√©quentiel effectu√©\")\n",
    "print(f\"  - Train: {len(X_train)} observations ({len(X_train)/len(X)*100:.1f}%)\")\n",
    "print(f\"  - Test: {len(X_test)} observations ({len(X_test)/len(X)*100:.1f}%)\")\n",
    "print(f\"  - P√©riode train: indices 0 √† {split_index-1}\")\n",
    "print(f\"  - P√©riode test: indices {split_index} √† {len(X)-1}\")\n",
    "\n",
    "# Visualisation du split\n",
    "fig, ax = plt.subplots(figsize=(15, 4))\n",
    "ax.plot(range(len(y)), y, alpha=0.6, linewidth=0.5, label='Donn√©es compl√®tes')\n",
    "ax.axvline(split_index, color='r', linestyle='--', linewidth=2, label=f'Split train/test (index {split_index})')\n",
    "ax.set_title('Visualisation du split train/test')\n",
    "ax.set_xlabel('Index')\n",
    "ax.set_ylabel('Consommation')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8f1e02e",
   "metadata": {},
   "source": [
    "### 5.4. Normalisation des features\n",
    "\n",
    "Standardisation des features pour que toutes les variables soient √† la m√™me √©chelle, ce qui am√©liore les performances des r√©seaux de neurones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4b5bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalisation avec StandardScaler (moyenne=0, √©cart-type=1)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Visualisation de l'impact de la normalisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Distribution d'une feature avant normalisation (ex: premi√®re variable m√©t√©o)\n",
    "feature_idx = 0\n",
    "axes[0].hist(X_train[:, feature_idx], bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0].set_title(f'Feature {feature_idx} avant normalisation')\n",
    "axes[0].set_xlabel('Valeur')\n",
    "axes[0].set_ylabel('Fr√©quence')\n",
    "axes[0].axvline(X_train[:, feature_idx].mean(), color='r', linestyle='--', \n",
    "                label=f'Moyenne: {X_train[:, feature_idx].mean():.2f}')\n",
    "axes[0].legend()\n",
    "\n",
    "# Distribution apr√®s normalisation\n",
    "axes[1].hist(X_train_scaled[:, feature_idx], bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
    "axes[1].set_title(f'Feature {feature_idx} apr√®s normalisation (StandardScaler)')\n",
    "axes[1].set_xlabel('Valeur normalis√©e')\n",
    "axes[1].set_ylabel('Fr√©quence')\n",
    "axes[1].axvline(X_train_scaled[:, feature_idx].mean(), color='r', linestyle='--', \n",
    "                label=f'Moyenne: {X_train_scaled[:, feature_idx].mean():.3f}')\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"‚úì Normalisation effectu√©e\")\n",
    "print(f\"  - Moyenne train (apr√®s scaling): {X_train_scaled.mean():.6f} (‚âà 0)\")\n",
    "print(f\"  - √âcart-type train (apr√®s scaling): {X_train_scaled.std():.6f} (‚âà 1)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a15a288",
   "metadata": {},
   "source": [
    "## 6. Mod√©lisation\n",
    "\n",
    "### 6.1. Configuration et entra√Ænement du mod√®le MLP\n",
    "\n",
    "Utilisation d'un Multi-Layer Perceptron (r√©seau de neurones) pour capturer les relations non-lin√©aires complexes entre les features et la consommation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c1a760",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration du mod√®le MLPRegressor\n",
    "mlp = MLPRegressor(\n",
    "    hidden_layer_sizes=(100, 100, 50),  # Architecture: 3 couches cach√©es\n",
    "    activation='relu',                   # Fonction d'activation ReLU\n",
    "    solver='adam',                       # Optimiseur Adam\n",
    "    max_iter=2000,                       # Nombre maximum d'it√©rations\n",
    "    early_stopping=True,                 # Arr√™t anticip√© si pas d'am√©lioration\n",
    "    n_iter_no_change=20,                # Nombre d'it√©rations sans am√©lioration avant arr√™t\n",
    "    alpha=0.5,                           # R√©gularisation L2\n",
    "    random_state=42                      # Pour la reproductibilit√©\n",
    ")\n",
    "\n",
    "print(\"Configuration du mod√®le MLP:\")\n",
    "print(f\"  - Architecture: {mlp.hidden_layer_sizes}\")\n",
    "print(f\"  - Activation: {mlp.activation}\")\n",
    "print(f\"  - Optimiseur: {mlp.solver}\")\n",
    "print(f\"  - R√©gularisation (alpha): {mlp.alpha}\")\n",
    "print(f\"  - Early stopping: {mlp.early_stopping}\")\n",
    "\n",
    "# Entra√Ænement du mod√®le\n",
    "print(\"\\n‚è≥ Entra√Ænement en cours...\")\n",
    "mlp.fit(X_train_scaled, y_train_log)\n",
    "print(\"‚úì Mod√®le entra√Æn√© avec succ√®s\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0388ef7a",
   "metadata": {},
   "source": [
    "### 6.2. Analyse de la courbe d'apprentissage\n",
    "\n",
    "Visualisation de l'√©volution de la loss pendant l'entra√Ænement pour d√©tecter le surapprentissage ou le sous-apprentissage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80df6d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©cup√©ration de la courbe de loss\n",
    "if hasattr(mlp, 'loss_curve_'):\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    ax.plot(mlp.loss_curve_, linewidth=2, color='steelblue')\n",
    "    ax.set_title('Courbe d\\'apprentissage - √âvolution de la loss')\n",
    "    ax.set_xlabel('It√©rations')\n",
    "    ax.set_ylabel('Loss (MSE)')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Marquer le point d'arr√™t si early stopping a √©t√© utilis√©\n",
    "    if mlp.n_iter_ < mlp.max_iter:\n",
    "        ax.axvline(mlp.n_iter_, color='r', linestyle='--', \n",
    "                   label=f'Arr√™t anticip√© √† l\\'it√©ration {mlp.n_iter_}')\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úì Nombre d'it√©rations effectu√©es: {mlp.n_iter_}\")\n",
    "    print(f\"  - Loss finale: {mlp.loss_curve_[-1]:.6f}\")\n",
    "    if len(mlp.loss_curve_) > 1:\n",
    "        print(f\"  - Am√©lioration: {((mlp.loss_curve_[0] - mlp.loss_curve_[-1]) / mlp.loss_curve_[0] * 100):.2f}%\")\n",
    "else:\n",
    "    print(\"‚ö† Courbe de loss non disponible (peut n√©cessiter validation_fraction > 0)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec82d3de",
   "metadata": {},
   "source": [
    "### üìä Interpr√©tation & Pistes d'am√©lioration - Mod√©lisation\n",
    "\n",
    "**Observations cl√©s :**\n",
    "- La courbe d'apprentissage montre si le mod√®le converge correctement\n",
    "- L'early stopping permet d'√©viter le surapprentissage\n",
    "- L'architecture (100, 100, 50) peut √™tre optimis√©e selon les performances\n",
    "\n",
    "**Pistes d'am√©lioration :**\n",
    "- Tester diff√©rentes architectures (plus profondes, plus larges, ou dropout)\n",
    "- Ajuster les hyperparam√®tres (learning_rate, alpha, batch_size)\n",
    "- Utiliser la validation crois√©e temporelle (TimeSeriesSplit) pour mieux √©valuer\n",
    "- Tester d'autres mod√®les (XGBoost, LSTM pour s√©ries temporelles, ensemble methods)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137adb1f",
   "metadata": {},
   "source": [
    "## 7. √âvaluation du Mod√®le\n",
    "\n",
    "### 7.1. Pr√©dictions et inversion de la transformation\n",
    "\n",
    "Les pr√©dictions sont faites sur l'√©chelle logarithmique, puis invers√©es pour revenir √† l'√©chelle originale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c986983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr√©dictions sur l'√©chelle logarithmique\n",
    "y_pred_test_log = mlp.predict(X_test_scaled)\n",
    "\n",
    "# Inversion de la transformation logarithmique\n",
    "y_pred_test = np.expm1(y_pred_test_log)  # expm1(x) = exp(x) - 1, inverse de log1p\n",
    "\n",
    "print(\"‚úì Pr√©dictions g√©n√©r√©es et transform√©es\")\n",
    "print(f\"  - Nombre de pr√©dictions: {len(y_pred_test)}\")\n",
    "print(f\"  - Min pr√©dit: {y_pred_test.min():.2f}, Max pr√©dit: {y_pred_test.max():.2f}\")\n",
    "print(f\"  - Min r√©el: {y_test_real.min():.2f}, Max r√©el: {y_test_real.max():.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206ae0f5",
   "metadata": {},
   "source": [
    "### 7.2. Calcul des m√©triques d'√©valuation\n",
    "\n",
    "Calcul des m√©triques principales pour √©valuer la performance du mod√®le : RMSE, MAE, et MAPE.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19ff7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul des m√©triques\n",
    "rmse_test = np.sqrt(mean_squared_error(y_test_real, y_pred_test))\n",
    "mae_test = mean_absolute_error(y_test_real, y_pred_test)\n",
    "mape_test = mean_absolute_percentage_error(y_test_real, y_pred_test) * 100\n",
    "\n",
    "# Affichage des r√©sultats\n",
    "print(\"=\" * 50)\n",
    "print(\"R√âSULTATS FINAUX - √âVALUATION SUR LE TEST SET\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"RMSE Test : {rmse_test:.2f} (Root Mean Squared Error)\")\n",
    "print(f\"MAE Test  : {mae_test:.2f} (Mean Absolute Error)\")\n",
    "print(f\"MAPE Test : {mape_test:.2f}% (Mean Absolute Percentage Error)\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Visualisation des m√©triques\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "metrics = ['RMSE', 'MAE', 'MAPE (%)']\n",
    "values = [rmse_test, mae_test, mape_test]\n",
    "colors = ['steelblue', 'coral', 'teal']\n",
    "\n",
    "bars = ax.bar(metrics, values, color=colors, alpha=0.7, edgecolor='black')\n",
    "ax.set_title('M√©triques d\\'√©valuation du mod√®le')\n",
    "ax.set_ylabel('Valeur')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for bar, val in zip(bars, values):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{val:.2f}', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57b686c",
   "metadata": {},
   "source": [
    "### 7.3. Visualisation des pr√©dictions vs valeurs r√©elles\n",
    "\n",
    "Comparaison visuelle des pr√©dictions avec les valeurs r√©elles pour identifier les patterns d'erreur.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5aab83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphique temporel : pr√©dictions vs r√©elles (zoom sur les 200 derni√®res heures)\n",
    "fig, axes = plt.subplots(2, 1, figsize=(15, 10))\n",
    "\n",
    "# Zoom sur 200 heures\n",
    "zoom_size = 200\n",
    "axes[0].plot(y_test_real[-zoom_size:], label=\"Valeurs r√©elles\", \n",
    "             color='black', alpha=0.7, linewidth=1.5)\n",
    "axes[0].plot(y_pred_test[-zoom_size:], label=f\"Pr√©dictions (MAPE: {mape_test:.2f}%)\", \n",
    "             color='red', linestyle='--', linewidth=1.5, alpha=0.8)\n",
    "axes[0].set_title(f'Comparaison Pr√©dictions vs R√©elles - Zoom sur {zoom_size} derni√®res heures')\n",
    "axes[0].set_xlabel('Heures (dans le test set)')\n",
    "axes[0].set_ylabel('Consommation')\n",
    "axes[0].legend(loc='best')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Erreurs r√©siduelles\n",
    "errors = y_test_real[-zoom_size:] - y_pred_test[-zoom_size:]\n",
    "axes[1].plot(errors, color='purple', alpha=0.7, linewidth=1)\n",
    "axes[1].axhline(0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[1].fill_between(range(len(errors)), errors, 0, alpha=0.3, color='purple')\n",
    "axes[1].set_title(f'Erreurs r√©siduelles - Zoom sur {zoom_size} derni√®res heures')\n",
    "axes[1].set_xlabel('Heures (dans le test set)')\n",
    "axes[1].set_ylabel('Erreur (R√©el - Pr√©dit)')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b3afc5",
   "metadata": {},
   "source": [
    "### 7.4. Scatter plot et analyse de corr√©lation\n",
    "\n",
    "Analyse de la corr√©lation entre pr√©dictions et valeurs r√©elles pour √©valuer la qualit√© du mod√®le.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a6104ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot : Pr√©dictions vs R√©elles\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Scatter plot avec ligne parfaite\n",
    "correlation = np.corrcoef(y_test_real, y_pred_test)[0, 1]\n",
    "axes[0].scatter(y_test_real, y_pred_test, alpha=0.5, color='blue', s=10)\n",
    "axes[0].plot([y_test_real.min(), y_test_real.max()], \n",
    "             [y_test_real.min(), y_test_real.max()], \n",
    "             'r--', lw=2, label='Ligne parfaite (y=x)')\n",
    "axes[0].set_xlabel('Valeurs R√©elles')\n",
    "axes[0].set_ylabel('Valeurs Pr√©dites')\n",
    "axes[0].set_title(f'Pr√©dictions vs R√©elles (Corr√©lation: {correlation:.3f}, MAPE: {mape_test:.2f}%)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Distribution des erreurs\n",
    "errors_all = y_test_real - y_pred_test\n",
    "axes[1].hist(errors_all, bins=50, edgecolor='black', alpha=0.7, color='coral')\n",
    "axes[1].axvline(0, color='r', linestyle='--', linewidth=2, label='Erreur = 0')\n",
    "axes[1].axvline(errors_all.mean(), color='g', linestyle='--', linewidth=2, \n",
    "                label=f'Moyenne: {errors_all.mean():.2f}')\n",
    "axes[1].set_title('Distribution des erreurs de pr√©diction')\n",
    "axes[1].set_xlabel('Erreur (R√©el - Pr√©dit)')\n",
    "axes[1].set_ylabel('Fr√©quence')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Statistiques des erreurs\n",
    "print(f\"\\nAnalyse des erreurs :\")\n",
    "print(f\"  - Erreur moyenne: {errors_all.mean():.2f} (id√©alement proche de 0)\")\n",
    "print(f\"  - √âcart-type des erreurs: {errors_all.std():.2f}\")\n",
    "print(f\"  - Corr√©lation pr√©dictions/r√©elles: {correlation:.4f}\")\n",
    "if abs(errors_all.mean()) < errors_all.std() * 0.1:\n",
    "    print(f\"  ‚úì Le mod√®le ne pr√©sente pas de biais significatif\")\n",
    "else:\n",
    "    print(f\"  ‚ö† Le mod√®le pr√©sente un biais (erreur moyenne ‚â† 0)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad10282b",
   "metadata": {},
   "source": [
    "### 7.5. Analyse des erreurs par contexte\n",
    "\n",
    "Analyse des performances selon diff√©rents contextes (heure, jour de la semaine, week-end) pour identifier les situations o√π le mod√®le performe moins bien.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf90658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction des contextes pour le test set\n",
    "test_heures = Heure_orig[168+split_index:]\n",
    "test_jours_semaine = jours_de_la_semaine[168+split_index:]\n",
    "test_is_weekend = is_weekend[168+split_index:]\n",
    "\n",
    "# Calcul du MAPE par contexte\n",
    "def calculate_mape_by_context(context_values, context_name):\n",
    "    \"\"\"Calcule le MAPE pour chaque valeur unique du contexte\"\"\"\n",
    "    unique_values = np.unique(context_values)\n",
    "    mape_by_context = {}\n",
    "    for val in unique_values:\n",
    "        mask = context_values == val\n",
    "        if mask.sum() > 0:\n",
    "            mape = mean_absolute_percentage_error(y_test_real[mask], y_pred_test[mask]) * 100\n",
    "            mape_by_context[val] = mape\n",
    "    return mape_by_context\n",
    "\n",
    "# MAPE par heure\n",
    "mape_by_hour = calculate_mape_by_context(test_heures, \"heure\")\n",
    "mape_by_weekday = calculate_mape_by_context(test_jours_semaine, \"jour_semaine\")\n",
    "\n",
    "# Visualisation\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# MAPE par heure\n",
    "hours = sorted(mape_by_hour.keys())\n",
    "mape_hours = [mape_by_hour[h] for h in hours]\n",
    "axes[0].plot(hours, mape_hours, marker='o', linewidth=2, markersize=6, color='steelblue')\n",
    "axes[0].axhline(mape_test, color='r', linestyle='--', label=f'MAPE moyen: {mape_test:.2f}%')\n",
    "axes[0].set_title('MAPE par heure de la journ√©e')\n",
    "axes[0].set_xlabel('Heure')\n",
    "axes[0].set_ylabel('MAPE (%)')\n",
    "axes[0].set_xticks(range(0, 24, 2))\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAPE par jour de la semaine\n",
    "jours_noms = ['Lun', 'Mar', 'Mer', 'Jeu', 'Ven', 'Sam', 'Dim']\n",
    "jours_idx = sorted(mape_by_weekday.keys())\n",
    "mape_days = [mape_by_weekday[j] for j in jours_idx]\n",
    "axes[1].bar(jours_noms, mape_days, color='coral', alpha=0.7)\n",
    "axes[1].axhline(mape_test, color='r', linestyle='--', label=f'MAPE moyen: {mape_test:.2f}%')\n",
    "axes[1].set_title('MAPE par jour de la semaine')\n",
    "axes[1].set_ylabel('MAPE (%)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Identification des heures/jours avec les meilleures/pires performances\n",
    "worst_hour = max(mape_by_hour, key=mape_by_hour.get)\n",
    "best_hour = min(mape_by_hour, key=mape_by_hour.get)\n",
    "worst_day = max(mape_by_weekday, key=mape_by_weekday.get)\n",
    "best_day = min(mape_by_weekday, key=mape_by_weekday.get)\n",
    "\n",
    "print(f\"\\nAnalyse des performances par contexte :\")\n",
    "print(f\"  - Meilleure heure: {best_hour}h (MAPE: {mape_by_hour[best_hour]:.2f}%)\")\n",
    "print(f\"  - Pire heure: {worst_hour}h (MAPE: {mape_by_hour[worst_hour]:.2f}%)\")\n",
    "print(f\"  - Meilleur jour: {jours_noms[best_day]} (MAPE: {mape_by_weekday[best_day]:.2f}%)\")\n",
    "print(f\"  - Pire jour: {jours_noms[worst_day]} (MAPE: {mape_by_weekday[worst_day]:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26a75e1",
   "metadata": {},
   "source": [
    "## 8. Analyse des R√©sultats et Conclusion\n",
    "\n",
    "### üìä Interpr√©tation & Pistes d'am√©lioration - √âvaluation\n",
    "\n",
    "**Observations cl√©s :**\n",
    "- **Performance globale** : Le MAPE, RMSE et MAE donnent une id√©e de la pr√©cision du mod√®le\n",
    "- **Distribution des erreurs** : Si centr√©e sur 0, le mod√®le n'a pas de biais syst√©matique\n",
    "- **Erreurs par contexte** : Identifier les heures/jours o√π le mod√®le performe moins bien permet de cibler les am√©liorations\n",
    "\n",
    "**Points forts identifi√©s :**\n",
    "- Les lags temporels (surtout lag_24h) sont tr√®s pr√©dictifs\n",
    "- Les encodages cycliques capturent bien les patterns saisonniers\n",
    "- Le mod√®le suit g√©n√©ralement bien les tendances\n",
    "\n",
    "**Points √† am√©liorer :**\n",
    "- Si certaines heures/jours ont un MAPE √©lev√©, cr√©er des features sp√©cifiques pour ces contextes\n",
    "- Si les erreurs sont corr√©l√©es (autocorr√©lation), consid√©rer des mod√®les de s√©ries temporelles (LSTM, ARIMA)\n",
    "- Tester des ensembles de mod√®les (stacking, blending) pour am√©liorer la robustesse\n",
    "\n",
    "**Pistes d'am√©lioration techniques :**\n",
    "1. **Feature engineering avanc√©** :\n",
    "   - Features d'interaction (m√©t√©o √ó saison, week-end √ó heure)\n",
    "   - Moyennes mobiles et tendances\n",
    "   - Features de pointe (d√©tection des pics de consommation)\n",
    "\n",
    "2. **Mod√©lisation** :\n",
    "   - Hyperparameter tuning (GridSearch/RandomSearch)\n",
    "   - Mod√®les de s√©ries temporelles (LSTM, GRU, Transformer)\n",
    "   - Ensemble methods (XGBoost, LightGBM, stacking)\n",
    "\n",
    "3. **Validation** :\n",
    "   - Validation crois√©e temporelle (TimeSeriesSplit)\n",
    "   - Validation sur plusieurs p√©riodes pour tester la robustesse\n",
    "\n",
    "4. **Post-processing** :\n",
    "   - Lissage des pr√©dictions si n√©cessaire\n",
    "   - Ajustement des pr√©dictions selon le contexte (calibrage)\n",
    "\n",
    "---\n",
    "\n",
    "**Notebook pr√™t pour it√©ration et am√©lioration continue ! üöÄ**\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
